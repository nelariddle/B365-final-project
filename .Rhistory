for (i in which(used == FALSE)) {
used[i] <- TRUE
XX <- cbind(X[, 1] ^ 0, X[, used])
a <- solve(t(XX) %*% XX , t(XX) %*% y)
yhat <- XX %*% a
error <- y - yhat
sse <- sum(error ^ 2)
if (sse < bestsse[j]) {
bestsse[j] <- sse
var[j] <- i
bestA <- a
}
used[i] <- FALSE
}
used[var[j]] <- TRUE
cat(bestA, "\n")
cat("the", ordinal(j), "best attribute is", colnames(X)[var[j]], '\n')
}
plot(bestsse,
main = "squared error vs. num attributes used",
xlab = "num attributes",
ylab = "squared error")
X <- instructor.data %>% select(
AVG.SECT.GPA,
TOTAL.INSTRUCTOR.GRADES,
YEAR,
COURSE.,
PERCENT.MAJORS,
AVG.INSTRUCTOR.GPA,
AVG.STDNT.CUM.GPA
) %>% as.matrix()
y <- instructor.data %>% select(NEWSALARY) %>% as.matrix()
n_attrs <- ncol(X)
used <-
rep(FALSE, n_attrs)
var <- rep(0, n_attrs)
bestsse <-
rep(10000000000000, n_attrs)
bestA <- rep(0, 3)
for (j in 1:n_attrs)  {
for (i in which(used == FALSE)) {
used[i] <- TRUE
XX <- cbind(X[, 1] ^ 0, X[, used])
a <- solve(t(XX) %*% XX , t(XX) %*% y)
yhat <- XX %*% a
error <- y - yhat
sse <- sum(error ^ 2)
if (sse < bestsse[j]) {
bestsse[j] <- sse
var[j] <- i
bestA <- a
}
used[i] <- FALSE
}
used[var[j]] <- TRUE
#(bestA, "\n")
cat("the", ordinal(j), "best attribute is", colnames(X)[var[j]], '\n')
}
plot(bestsse,
main = "squared error vs. num attributes used",
xlab = "num attributes",
ylab = "squared error")
X <- instructor.data %>% select(
AVG.SECT.GPA,
TOTAL.INSTRUCTOR.GRADES,
YEAR,
COURSE.,
PERCENT.MAJORS,
AVG.INSTRUCTOR.GPA,
AVG.STDNT.CUM.GPA
) %>% as.matrix()
y <- instructor.data %>% select(NEWSALARY) %>% as.matrix()
n_attrs <- ncol(X)
used <-
rep(FALSE, n_attrs)
var <- rep(0, n_attrs)
bestsse <-
rep(10000000000000, n_attrs)
bestA <- rep(0, 3)
for (j in 1:n_attrs)  {
for (i in which(used == FALSE)) {
used[i] <- TRUE
XX <- cbind(X[, 1] ^ 0, X[, used])
a <- solve(t(XX) %*% XX , t(XX) %*% y)
yhat <- XX %*% a
error <- y - yhat
sse <- sum(error ^ 2)
if (sse < bestsse[j]) {
bestsse[j] <- sse
var[j] <- i
bestA <- a
}
used[i] <- FALSE
}
used[var[j]] <- TRUE
cat(bestA, "\n")
cat("the", ordinal(j), "best attribute is", colnames(X)[var[j]], '\n')
}
X <- instructor.data %>% select(
AVG.SECT.GPA,
TOTAL.INSTRUCTOR.GRADES,
YEAR,
COURSE.,
PERCENT.MAJORS,
AVG.INSTRUCTOR.GPA,
AVG.STDNT.CUM.GPA
) %>% as.matrix()
y <- instructor.data %>% select(NEWSALARY) %>% as.matrix()
n_attrs <- ncol(X)
used <-
rep(FALSE, n_attrs)
var <- rep(0, n_attrs)
bestsse <-
rep(10000000000000, n_attrs)
bestA <- rep(0, 3)
for (j in 1:n_attrs)  {
for (i in which(used == FALSE)) {
used[i] <- TRUE
XX <- cbind(X[, 1] ^ 0, X[, used])
a <- solve(t(XX) %*% XX , t(XX) %*% y)
yhat <- XX %*% a
error <- y - yhat
sse <- sum(error ^ 2)
if (sse < bestsse[j]) {
bestsse[j] <- sse
var[j] <- i
bestA <- a
}
used[i] <- FALSE
}
used[var[j]] <- TRUE
# cat(bestA, "\n")
cat("the", ordinal(j), "best attribute is", colnames(X)[var[j]], '\n')
}
data<-read.csv("data-with-inflation.csv",stringsAsFactors = TRUE)
data<-read.csv("../data-with-inflation.csv",stringsAsFactors = TRUE)
data<-read.csv("../data/data-with-inflation.csv",stringsAsFactors = TRUE)
setwd("../data")
data<-read.csv("data-with-inflation.csv",stringsAsFactors = TRUE)
setwd("~/fall 2022/b365/Final project/code referenced in paper")
setwd("../data")
data<-read.csv("data-with-inflation.csv",stringsAsFactors = TRUE)
setwd("~/fall 2022/b365/Final project/code referenced in paper")
data<-read.csv("data-with-inflation.csv",stringsAsFactors = TRUE)
setwd("~")
printwd()
getwd()
getwd()
data<-read.csv("data-with-inflation.csv",stringsAsFactors = TRUE)
data<-read.csv("data/data-with-inflation.csv",stringsAsFactors = TRUE)
data<-read.csv("../data/data-with-inflation.csv",stringsAsFactors = TRUE)
data<-read.csv("/../data/data-with-inflation.csv",stringsAsFactors = TRUE)
setwd(getSrcDirectory()[1])
here()
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library("rstudioapi")
install.packages("rstudioapi")
library("rstudioapi")
setwd(dirname(getActiveDocumentContext()$path))
setwd(getSrcDirectory()[1])
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# install.packages("dplyr")
# install.packages("english")
library(dplyr)
data<-read.csv("data-with-inflation.csv",stringsAsFactors = TRUE)
library(ggplot2)
library(dplyr)
## Data cleaning Nela did:
# dont care about career and thesis courses
data<-data%>%filter(LETTER!="Y"&LETTER!="G")
# only undergrad
data<-data%>%filter(COURSE.<500)
## More Data cleaning
# Select only important columns
data = data[,c("YEAR", "COURSE.", "PERCENT.MAJORS", "NEWSALARY", "AVG.SECT.GPA")]
# Only use if you want to put the course numbers into bins
#data["COURSE."] = floor(data["COURSE."] / 100)
# Test linear model with each of the variables for the best model
allSE = rep(0, 4)
for(i in 1:(length(data[1,])-1)){
d = cbind(rep(1,length(data[,1])), data[,i])
a = solve(t(d) %*% d, t(d) %*% data[,length(data)])
yhat = d %*% a
error = data[,5] - yhat
se = sum(error ^ 2)
allSE[i] = se
print(paste("Squared Error for", colnames(data)[i], "is", round(se, 3)))
}
# Printing out the order of how good the predictors were
ordered = order(allSE)
print(paste("The best predictor for GPA is", colnames(data)[ordered[1]], "with squared error", allSE[ordered[1]]))
print(paste("The second best predictor for GPA is", colnames(data)[ordered[2]], "with squared error", allSE[ordered[2]]))
print(paste("The third best predictor for GPA is", colnames(data)[ordered[3]], "with squared error", allSE[ordered[3]]))
print(paste("The worst predictor for GPA is", colnames(data)[ordered[4]], "with squared error", allSE[ordered[4]]))
# Bar chart of the SSE for the single variable regression
# Not used in paper or presentation
ggplot() +
geom_bar(aes(x = (colnames(data)[1:4]), y = allSE, fill = (colnames(data)[1:4])), stat = "identity") +
geom_text(aes(x = (colnames(data)[1:4]), y = allSE, label = round(allSE, 2)), vjust = 2)
# Test linear model for 2nd best predictor
# Not important because the one below does the same thing but better
# Only used if you want to do the 2-variable graph because the one below
#     isn't set up to calculate yhat2 or the equation
for(i in 1:4){
if(i != 1){
d = cbind(rep(1,length(data[,1])), data[,i], data[,1])
a = solve(t(d) %*% d, t(d) %*% data[,length(data)])
yhat2 = d %*% a
error = data[,5] - yhat2
se = sum(error ^ 2)
print(paste("Squared Error for year and", colnames(data)[i], "is", round(se, 3)))
}
}
print("Second best predictor combined with year is course number")
print(paste("The linear model for the effect of year and course number on GPA is ",
round(a[1], 7)," + ", round(a[2], 7),"[COURSE.] + ", round(a[3], 7), "[YEAR]", sep = ""))
# Not using this plot- ggplot is just better
#plot(data[,"COURSE."], data[,"AVG.SECT.GPA"])
#lines(sort(data[,"COURSE."]), sort(yhat2))
# Creating the ggplot of the linear relationship between GPA and year
#     (with course number as the color)
ggplot() +
geom_point(data = data, aes(x = data[,"YEAR"], y = data[,"AVG.SECT.GPA"], color = data[,"COURSE."])) +
geom_smooth(aes(x = sort(data[,"YEAR"]), y = sort(yhat)), method = "lm") +
labs(color = "Course Number", title = "Effect of Year on GPA", x = "Year",
y = "Average Student GPA")
# Linear relationship with both year and course number considered
ggplot() +
geom_point(data = data, aes(x = data[,"YEAR"], y = data[,"AVG.SECT.GPA"], color = data[,"COURSE."])) +
geom_smooth(aes(x = sort(data[,"YEAR"]), y = sort(yhat)), method = "lm") +
geom_smooth(aes(x = sort(data[,"YEAR"]), y = sort(yhat2)), method = "lm", color = "red") +
labs(color = "Course Number", title = "Effect of Year on GPA", x = "Year",
y = "Average Student GPA")
# Finding the graph of the SSE as the number of attributes used increases
bestSSE = rep(0,4)
n = length(data[,1])
ncol = length(data) - 1
used = rep(0, (n-1))
prev = rep(1,length(data[,1]))
for(j in 1:ncol){
allSE = rep(0, 4)
for(i in 1:ncol){
if(sum(used == i) == 0){
d = cbind(prev, data[,i])
a = solve(t(d) %*% d, t(d) %*% data[,length(data)])
yhat = d %*% a
error = data[,5] - yhat
se = sum(error ^ 2)
allSE[i] = se
print(paste("Squared Error for", colnames(data)[i], "is", round(se, 3)))
}
}
o = order(allSE)
used[j] = o[j]
prev = cbind(prev, data[,o[j]])
bestSSE[j] = allSE[o[j]]
print(paste("Attribute",j, "is", colnames(data)[o[j]]))
}
print(bestSSE)
# The graph of the change in SSE as more attributes are added
ggplot() +
geom_point(aes(x = 1:4, y = bestSSE)) +
geom_smooth(aes(x = 1:4, y = bestSSE), method = "lm") +
xlab("Number of attributes used") +
ylab("Sum of squared error") +
ggtitle("Sum of squared error predicting GPA of a class")
setwd("~/fall 2022/b365/Final project")
data<-read.csv("data-with-inflation.csv",stringsAsFactors = TRUE)
library(ggplot2)
library(dplyr)
## Data cleaning Nela did:
# dont care about career and thesis courses
data<-data%>%filter(LETTER!="Y"&LETTER!="G")
# only undergrad
data<-data%>%filter(COURSE.<500)
## More Data cleaning
# Select only important columns
data = data[,c("YEAR", "COURSE.", "PERCENT.MAJORS", "NEWSALARY", "AVG.SECT.GPA")]
# Only use if you want to put the course numbers into bins
#data["COURSE."] = floor(data["COURSE."] / 100)
# Test linear model with each of the variables for the best model
allSE = rep(0, 4)
for(i in 1:(length(data[1,])-1)){
d = cbind(rep(1,length(data[,1])), data[,i])
a = solve(t(d) %*% d, t(d) %*% data[,length(data)])
yhat = d %*% a
error = data[,5] - yhat
se = sum(error ^ 2)
allSE[i] = se
print(paste("Squared Error for", colnames(data)[i], "is", round(se, 3)))
}
# Printing out the order of how good the predictors were
ordered = order(allSE)
print(paste("The best predictor for GPA is", colnames(data)[ordered[1]], "with squared error", allSE[ordered[1]]))
print(paste("The second best predictor for GPA is", colnames(data)[ordered[2]], "with squared error", allSE[ordered[2]]))
print(paste("The third best predictor for GPA is", colnames(data)[ordered[3]], "with squared error", allSE[ordered[3]]))
print(paste("The worst predictor for GPA is", colnames(data)[ordered[4]], "with squared error", allSE[ordered[4]]))
# Bar chart of the SSE for the single variable regression
# Not used in paper or presentation
ggplot() +
geom_bar(aes(x = (colnames(data)[1:4]), y = allSE, fill = (colnames(data)[1:4])), stat = "identity") +
geom_text(aes(x = (colnames(data)[1:4]), y = allSE, label = round(allSE, 2)), vjust = 2)
# Test linear model for 2nd best predictor
# Not important because the one below does the same thing but better
# Only used if you want to do the 2-variable graph because the one below
#     isn't set up to calculate yhat2 or the equation
for(i in 1:4){
if(i != 1){
d = cbind(rep(1,length(data[,1])), data[,i], data[,1])
a = solve(t(d) %*% d, t(d) %*% data[,length(data)])
yhat2 = d %*% a
error = data[,5] - yhat2
se = sum(error ^ 2)
print(paste("Squared Error for year and", colnames(data)[i], "is", round(se, 3)))
}
}
print("Second best predictor combined with year is course number")
print(paste("The linear model for the effect of year and course number on GPA is ",
round(a[1], 7)," + ", round(a[2], 7),"[COURSE.] + ", round(a[3], 7), "[YEAR]", sep = ""))
# Not using this plot- ggplot is just better
#plot(data[,"COURSE."], data[,"AVG.SECT.GPA"])
#lines(sort(data[,"COURSE."]), sort(yhat2))
# Creating the ggplot of the linear relationship between GPA and year
#     (with course number as the color)
ggplot() +
geom_point(data = data, aes(x = data[,"YEAR"], y = data[,"AVG.SECT.GPA"], color = data[,"COURSE."])) +
geom_smooth(aes(x = sort(data[,"YEAR"]), y = sort(yhat)), method = "lm") +
labs(color = "Course Number", title = "Effect of Year on GPA", x = "Year",
y = "Average Student GPA")
# Linear relationship with both year and course number considered
ggplot() +
geom_point(data = data, aes(x = data[,"YEAR"], y = data[,"AVG.SECT.GPA"], color = data[,"COURSE."])) +
geom_smooth(aes(x = sort(data[,"YEAR"]), y = sort(yhat)), method = "lm") +
geom_smooth(aes(x = sort(data[,"YEAR"]), y = sort(yhat2)), method = "lm", color = "red") +
labs(color = "Course Number", title = "Effect of Year on GPA", x = "Year",
y = "Average Student GPA")
# Finding the graph of the SSE as the number of attributes used increases
bestSSE = rep(0,4)
n = length(data[,1])
ncol = length(data) - 1
used = rep(0, (n-1))
prev = rep(1,length(data[,1]))
for(j in 1:ncol){
allSE = rep(0, 4)
for(i in 1:ncol){
if(sum(used == i) == 0){
d = cbind(prev, data[,i])
a = solve(t(d) %*% d, t(d) %*% data[,length(data)])
yhat = d %*% a
error = data[,5] - yhat
se = sum(error ^ 2)
allSE[i] = se
print(paste("Squared Error for", colnames(data)[i], "is", round(se, 3)))
}
}
o = order(allSE)
used[j] = o[j]
prev = cbind(prev, data[,o[j]])
bestSSE[j] = allSE[o[j]]
print(paste("Attribute",j, "is", colnames(data)[o[j]]))
}
print(bestSSE)
# The graph of the change in SSE as more attributes are added
ggplot() +
geom_point(aes(x = 1:4, y = bestSSE)) +
geom_smooth(aes(x = 1:4, y = bestSSE), method = "lm") +
xlab("Number of attributes used") +
ylab("Sum of squared error") +
ggtitle("Sum of squared error predicting GPA of a class")
setwd("C:/Users/nelar/OneDrive/Documents/fall 2022/b365/Final project")
data <-
read.csv("data-with-salary-class.csv", stringsAsFactors = TRUE)
# install.packages("dplyr")
# install.packages("english")
library(dplyr)
library(english)
# add a column for percent GPA grades
data$PCT.GPA <- data$GPA.GRADES / data$TOTAL.GRADES
# dont care about career and thesis courses
data <- data %>% filter(LETTER != "Y" & LETTER != "G")
# only undergrad
data <- data %>% filter(COURSE. < 500)
# create instructor data
# TOTAL.GRADES.INSTRUCTOR is the total # of grades given by an instructor that year
# AVG.GPA INSTRUCTOR is the average GPA of grades given by an instructor that year
instructor.data <-
data %>% group_by(INSTRUCTOR.NAME, YEAR) %>% mutate(
TOTAL.INSTRUCTOR.GRADES =
sum(TOTAL.GRADES),
AVG.INSTRUCTOR.GPA =
sum(AVG.SECT.GPA *
TOTAL.GRADES) /
sum(TOTAL.GRADES)
) %>% ungroup()
# single variable analysis
library(english)
X <-
instructor.data %>% select(
TOTAL.INSTRUCTOR.GRADES,
AVG.SECT.GPA,
YEAR,
COURSE.,
PERCENT.MAJORS,
AVG.INSTRUCTOR.GPA,
AVG.STDNT.CUM.GPA
) %>% as.matrix()
y <- instructor.data %>% select(NEWSALARY) %>% as.matrix()
n_attrs <- ncol(X)
sses <- rep(0, n_attrs)
attr_perf <- array(0, dim = c(n_attrs, 4))
for (i in 1:n_attrs) {
plot(X[, i], y, xlab = colnames(X)[i], ylab = colnames(y)[1])
d <- cbind(X[, i] ^ 0, X[, i] ^ 1)
a <- solve(t(d) %*% d, t(d) %*% y)
abline(a[1], a[2], col = "purple")
yhat <- d %*% a
error <- y - yhat
sse <- sum(error ^ 2)
sses[i] <- sse
attr_perf[i,] <- c(colnames(X)[i], round(sse, 3), a[1], a[2])
}
for (rank in order(sses)) {
cat(
attr_perf[rank, 1],
"\nerror:",
round(as.numeric(attr_perf[rank, 2], 3)),
"intercept:",
round(as.numeric (attr_perf[rank, 3]), 3),
"slope:",
round  (as.numeric (attr_perf[rank, 4]), 3),
"\n"
)
}
# variable selection
X <- instructor.data %>% select(
AVG.SECT.GPA,
TOTAL.INSTRUCTOR.GRADES,
YEAR,
COURSE.,
PERCENT.MAJORS,
AVG.INSTRUCTOR.GPA,
AVG.STDNT.CUM.GPA
) %>% as.matrix()
y <- instructor.data %>% select(NEWSALARY) %>% as.matrix()
n_attrs <- ncol(X)
used <-
rep(FALSE, n_attrs)
var <- rep(0, n_attrs)
bestsse <-
rep(10000000000000, n_attrs)
bestA <- rep(0, 3)
for (j in 1:n_attrs)  {
for (i in which(used == FALSE)) {
used[i] <- TRUE
XX <- cbind(X[, 1] ^ 0, X[, used])
a <- solve(t(XX) %*% XX , t(XX) %*% y)
yhat <- XX %*% a
error <- y - yhat
sse <- sum(error ^ 2)
if (sse < bestsse[j]) {
bestsse[j] <- sse
var[j] <- i
bestA <- a
}
used[i] <- FALSE
}
used[var[j]] <- TRUE
# cat(bestA, "\n")
cat("the", ordinal(j), "best attribute is", colnames(X)[var[j]], '\n')
}
plot(bestsse,
main = "squared error vs. num attributes used",
xlab = "num attributes",
ylab = "squared error")
# Code used from https://www.geeksforgeeks.org/naive-bayes-classifier-in-r-programming/
# Installing Packages
install.packages("e1071")
install.packages("caTools")
install.packages("caret")
# Loading package
library(e1071)
library(caTools)
library(caret)
data = read.csv("data-with-salary-class.csv")
dimnames(data)
library(dplyr)
# Data cleaning from Nela
# add a column for percent GPA grades
data$PCT.GPA <- data$GPA.GRADES / data$TOTAL.GRADES
# dont care about career and thesis courses
data <- data %>% filter(LETTER != "Y" & LETTER != "G")
# only undergrad
data <- data %>% filter(COURSE. < 500)
## More Data cleaning
# Select only important columns
data = data[,c("COURSE.", "PERCENT.MAJORS", "AVG.SECT.GPA", "SALARYCLASS")]
# Flooring the course numbers to only 1,2,3, and 4
data["COURSE."] = floor(data["COURSE."] / 100)
split <- sample.split(data, SplitRatio = 0.5)
train <- subset(data, split == "TRUE")
test <- subset(data, split == "FALSE")
train
test
#train_scale <- scale(train[, 5])
#test_scale <- scale(test[, 5])
set.seed(120)
classifier_cl <- naiveBayes(SALARYCLASS ~ ., data = train)
classifier_cl
y_pred <- predict(classifier_cl, newdata = test)
cm <- table(test$SALARYCLASS, y_pred)
cm
confusionMatrix(cm)
